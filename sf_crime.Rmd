---
title: "Predicting Crime in San Francisco"
author: "Matt Boensel, Alcides Sorto, Edgar Sandoval"
date: "May 12, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Motivation
For our project, we wanted to determine if there was a viable method to model crime in San Francisco. More specifically, we wanted to see if it was possible to predict where a crime was most likely to occur given geospatial features, time, and weather conditions. Crime can be thought of as an air bubble under a rug. It is easy to smooth out the area, but the bubble will oftentimes simply travel to another side of the rug. With predictive analysis of crime, it is possible to determine where where this bubble will appear before smoothing out the previous bump, and thus get the jump on things. Predicting crime is of great interest to society because even a small reduction in crime can lead to a large increase in quality of life of all effected residents. 

# Reading Data
Below we read in the data and load necessary libraries and functions.  Our project drew from several sources in order to obtain the requisite data on weather conditions, type of crime, and geospatial features.  

```{r reading in data, message=FALSE, warning=FALSE}
library(sp)
library(rgdal)
library(maps)
library(raster)
library(maptools)
library(ggplot2)
library(ggmap)
library(ROCR)
library(e1071)

source("functions.r")
source("lin-regr-util.R")

crime.data <- read.csv("SFPD_Incidents_-_Previous_Year__2015_.csv")
polygon.data <- readOGR(dsn=path.expand("sf-tracts"), layer="sf-tracts")
weather.data <- read.csv("weatherData.csv")

options(digits=16)
names = polygon.data@data$NAME10

```

# Preprocessing data
## Enriching with Geospatial Data
The U.S. Census separates cities into tracts, of which San Francisco has 197. We decided to add these tracts to our data as a feature. To do this, we located a shape file containing polygons corresponding to the census tracts of San Francisco. Then we determined which tract each crime occurred in and added that data as a feature.

```{r preprocessing geospatial data, message=FALSE, warning=FALSE}
#Combining geospatial data
crime.data.locations <- crime.data[,c("Location")]

#extract higher accuracy geo location from the crime data file
res = sapply(crime.data.locations, function(z){
  mach <- gregexpr("-?[[:digit:]]+\\.[[:digit:]]+", z)
  lat = latLong(z,mach,1)
  lng = latLong(z,mach,2)
  return (c(lat,lng))
})

#prepare points and polygons to match each crime location to a tract.
res = t(res)
res = as.data.frame(res)
crime.data.locations = data.frame(crime.data.locations, res)
names(crime.data.locations) <- c("Location","lat","lng")

point.y <- crime.data.locations[,"lng"]
point.x <- crime.data.locations[,"lat"]

#Match crime point to a tract and create a data frame.
points.in <- sapply(polygon.data@polygons, function(poly){
  pol.y = poly@Polygons[[1]]@coords[,1]#lon
  pol.x = poly@Polygons[[1]]@coords[,2]#lat
  return(point.in.polygon(point.x,point.y,pol.x,pol.y, mode.checked = F))
})

points.in <- as.data.frame(points.in)

names(points.in) <- names

crime.point.which <- apply(points.in,1,function(x){ x==1})

drop <- apply(points.in, 1, function(x){sum(x)==1})

crime.data.locations <- crime.data.locations[drop,]

crime.point.which = t(crime.point.which)

crime.point.which.tract <- (unlist(apply(crime.point.which,1,function(x){ as.character(names[x]) })))

#Combine tract data with crime data file
crime.data.locations = data.frame(crime.data.locations,Tract=crime.point.which.tract)

crime.data <- crime.data[drop,]

row.names(crime.data.locations) = NULL
crime.data.locations$Location = NULL

crime.data <- data.frame(crime.data,crime.data.locations)

crime.data$Y = NULL
crime.data$X = NULL
crime.data$Location = NULL
```

## Enriching with weather data
While there are many websites that offer APIs to obtain historical weather data, we were unable to find one which did so without charging an exorbitant rate.  As such, we had to fall back on screen scraping to obtain the 2015 weather data for San Francisco. Using the Requests Python library, we created a CSV of the year's weather observations taken hourly at San Francisco International Airport (SFO). After some formatting changes, we added several features of interest to our data.  
```{r weather data preprocessing, message=FALSE, warning=FALSE}

#Take only temp/humidity/date+time/notable conditions from csv
weather.data <- weather.data[,c("TemperatureF","Conditions","Humidity","Events","date","changedTime")]

#change date format to m/d/y
weather.data$date <- as.Date(weather.data$date,format="%Y-%m-%d")
weather.data$date <- format(weather.data$date, "%m/%d/%Y")

#clean crime.data formatting
dates = unlist(lapply(as.character(crime.data$Date), function(x) strsplit(x, "\\s+")[[1]][1]))

crime_dates = as.Date(dates,format = "%m/%d/%Y")
crime.data$Date = format(crime_dates, "%m/%d/%Y")

#remove minutes, as we are pairing rows on the hour
hours = substr(crime.data$Time, 1,2)
crime.data$Hour = as.numeric(hours)

#rename columns
names(weather.data) = c("TemperatureF", "Conditions" ,  "Humidity"   ,  "Events"   ,    "Date"       ,  "Hour" ) 

#perform join on matching features
test <- merge(crime.data, weather.data, all.x=T)

#discard rows that do not have weather data
crime.data = test[!is.na(test$Humidity),]

summary(crime.data)

```

# Data Exploration and Visualization
We wanted to examine the categories of crimes with regards to a variety of factors. Below you will find the distribution of crimes by day of week, month, and hour. In doing so we hoped to discover trends in when certain crimes occur most often. Following this exploration, we generated heatmaps that display where crimes were committed in San Francisco, with the relevant census tract overlayed.

```{r data visualization, message=FALSE, warning=FALSE, results="hide"}

polygon.data <- spTransform(polygon.data, CRS("+proj=longlat +datum=WGS84"))
polygon.data <- fortify(polygon.data)
par(mar=c(5,6,3,2))
crime.data$Date = as.Date(crime.data$Date,format = "%m/%d/%Y")

#plots the number of crimes reported per day of the week in jan-dec
months = unique(format(crime.data$Date, "%B"))
par(mar=c(5,6,3,2))
sapply(months, function(m) 
  barplot(table(format(crime.data$Date[format(crime.data$Date, "%B")==m], "%A")),
                                   las=2, horiz = T, main = paste("Crime for", m, "by day of the week"), xlab = "Frequency"))
#default margins
par(mar=c(5.1, 4.1, 4.1, 2.1))

#plots the categories of crimes
par(mar=c(4, 6, 1, 1)+.1)
crime_categories = unique(crime.data$Category)

#plot categories of crimes per month per day of the week
par(cex.axis=1, cex.lab=1, cex.main=1.2, cex.sub=1)
sapply(months, function(m)
  barplot(table(crime.data[format(crime.data$Date, "%B") == m,]$Category), 
          las=2, horiz=T,cex.names=.35, cex.axis = .5, main = paste("Crime Counts for", m), xlab="Frequency"))

hours = substr(crime.data$Time, 1,2)
crime.data$Hour = hours
hours=unique(hours)


#displays the times when crimes occur given their category
par(mar=c(4, 4, 2, 1)+.1)
sapply(crime_categories, function(c)
  barplot(table(crime.data[crime.data$Category == c,]$Hour), main = paste("Times when", c, "has been reported"), xlab = "Hour", ylab="Frequency"))



mean.latitude = mean(crime.data$lat)
mean.longitude = mean(crime.data$lng)

crimes_split = split(as.character(crime_categories),ceiling(seq_along(as.character(crime_categories))/5))

split1 = unlist(crimes_split[1])
split2 = unlist(crimes_split[2])
split3 = unlist(crimes_split[3])
split4 = unlist(crimes_split[4])
split5 = unlist(crimes_split[5])
split6 = unlist(crimes_split[6])
split7 = unlist(crimes_split[7])
split8 = unlist(crimes_split[8])

san_fran = qmap(location=c(mean.longitude, mean.latitude), zoom = 13)
san_fran + stat_density2d(aes(x = lng, y = lat, fill = ..level.., alpha = ..level..), geom ="polygon",data = crime.data[crime.data$Category %in% split1,]) + scale_fill_gradient(low = "yellow", high = "red") + facet_wrap(~ Category) + ggtitle("Locations crimes were reported") + geom_polygon(aes(x=long, y=lat, group=group), fill=NA, size=.2,color='black', data=polygon.data, alpha=.5)

san_fran = qmap(location=c(mean.longitude, mean.latitude), zoom = 13)
san_fran + stat_density2d(aes(x = lng, y = lat, fill = ..level.., alpha = ..level..), geom ="polygon",data = crime.data[crime.data$Category %in% split2,]) + scale_fill_gradient(low = "yellow", high = "red") + facet_wrap(~ Category) + ggtitle("Locations crimes were reported") + geom_polygon(aes(x=long, y=lat, group=group), fill=NA, size=.2,color='black', data=polygon.data, alpha=.5)

san_fran = qmap(location=c(mean.longitude, mean.latitude), zoom = 13)
san_fran + stat_density2d(aes(x = lng, y = lat, fill = ..level.., alpha = ..level..), geom ="polygon",data = crime.data[crime.data$Category %in% split3,]) + scale_fill_gradient(low = "yellow", high = "red") + facet_wrap(~ Category) + ggtitle("Locations crimes were reported") + geom_polygon(aes(x=long, y=lat, group=group), fill=NA, size=.2,color='black', data=polygon.data, alpha=.5)

san_fran = qmap(location=c(mean.longitude, mean.latitude), zoom = 13)
san_fran + stat_density2d(aes(x = lng, y = lat, fill = ..level.., alpha = ..level..), geom ="polygon",data = crime.data[crime.data$Category %in% split4,]) + scale_fill_gradient(low = "yellow", high = "red") + facet_wrap(~ Category) + ggtitle("Locations crimes were reported") + geom_polygon(aes(x=long, y=lat, group=group), fill=NA, size=.2,color='black', data=polygon.data, alpha=.5)

san_fran = qmap(location=c(mean.longitude, mean.latitude), zoom = 13)
san_fran + stat_density2d(aes(x = lng, y = lat, fill = ..level.., alpha = ..level..), geom ="polygon",data = crime.data[crime.data$Category %in% split5,]) + scale_fill_gradient(low = "yellow", high = "red") + facet_wrap(~ Category) + ggtitle("Locations crimes were reported") + geom_polygon(aes(x=long, y=lat, group=group), fill=NA, size=.2,color='black', data=polygon.data, alpha=.5)

san_fran = qmap(location=c(mean.longitude, mean.latitude), zoom = 13)
san_fran + stat_density2d(aes(x = lng, y = lat, fill = ..level.., alpha = ..level..), geom ="polygon",data = crime.data[crime.data$Category %in% split6,]) + scale_fill_gradient(low = "yellow", high = "red") + facet_wrap(~ Category) + ggtitle("Locations crimes were reported") + geom_polygon(aes(x=long, y=lat, group=group), fill=NA, size=.2,color='black', data=polygon.data, alpha=.5)

san_fran = qmap(location=c(mean.longitude, mean.latitude), zoom = 13)
san_fran + stat_density2d(aes(x = lng, y = lat, fill = ..level.., alpha = ..level..), geom ="polygon",data = crime.data[crime.data$Category %in% split7,]) + scale_fill_gradient(low = "yellow", high = "red") + facet_wrap(~ Category) + ggtitle("Locations crimes were reported") + geom_polygon(aes(x=long, y=lat, group=group), fill=NA, size=.2,color='black', data=polygon.data, alpha=.5)

san_fran = qmap(location=c(mean.longitude, mean.latitude), zoom = 13)
san_fran + stat_density2d(aes(x = lng, y = lat, fill = ..level.., alpha = ..level..), geom ="polygon",data = crime.data[crime.data$Category %in% split8,]) + scale_fill_gradient(low = "yellow", high = "red") + facet_wrap(~ Category) + ggtitle("Locations crimes were reported") + geom_polygon(aes(x=long, y=lat, group=group), fill=NA, size=.2,color='black', data=polygon.data, alpha=.5)
```

# Building Models

## Logistic Regression
Our first attempt to model crime utilizes logistic regression. In this model, we explored whether we could accurately predict the occurence of a specific crime type given weather, time, and geospatial features. We added a column, output, to crime.data that was a binary indicator variable. Output = 1 for each row which corresponds to an occurence of our specific type of crime, and 0 otherwise. We chose to first look at robberies, as they are a type of crime that is of particular concern to the typical citizen.

```{r logistic regression, message=FALSE, warning=FALSE}

#create column for output class
crime.data$output =  as.numeric(crime.data$Category == "ROBBERY")

#split data into training and test sets
set.seed(123)
data.split = split_data(crime.data)

tr_dat = data.split[[1]]
te_dat = data.split[[2]]

#generate model based on weather events, conditions, temp, day, and hour
fit = glm(output ~ Events + Humidity + Conditions + TemperatureF + DayOfWeek + Hour, data = tr_dat, family = binomial)

summary(fit)

#make predictions using test data
predicted = predict(fit, newdata=te_dat , type = "response")

#double density plot
plot(density(predicted[te_dat$output == 0]), col="green", main="Double Density Plot")
lines(density(predicted[te_dat$output == 1]), col="red")
thresh = .025
abline(v=thresh, lty=2, col="blue")

predicted_vals = as.numeric(predicted < thresh)
actuals = te_dat$output

#generate and display confusion matrix
conf_matrix = table(actuals, predicted_vals)

conf_matrix

accuracy = mean(actuals == predicted_vals)

paste("Model Accuracy =", accuracy)
```

Unfortunately, our model did not do very well. We got a fairly low accuracy with the threshold we chose and both of the curves on the double density plot had relatively the same shape and size. We believe that the model didn't perform well because our methodology may have been a bit too narrow in its focus. We compared robberies to all other crimes, and it is very possible that those other crimes occured under the same circumstances as robberies in San Francisco. Additionally, several other categories seem like they may have characteristics that overlap with robbery, so looking at robberies as a distinct entity is of questionable value. 


### Model Analysis
The output from the receiver operating characteristic, accuracy threshold, and precision recall curves showed little promise and solidified that our model needed more work. 

```{r analysis of the logistic model, message=FALSE, warning=FALSE}
crime_predictions = prediction(predicted, te_dat$output)
roc = performance(crime_predictions, measure = "tpr", x.measure = "fpr")
prec_recall = performance(crime_predictions, measure="prec", x.measure="rec")
acc_curve = performance(crime_predictions, measure = "acc", x.measure = "cutoff")


plot(acc_curve, main="Accuracy Threshold Curve")
plot(prec_recall, main = "Precision Recall Curve")
plot(roc, main="Receiver Operating Characteristic",col="blue")
```

### Model Reevaluation
To move forward with our logistical regression model, we explored considering more category values to be valid occurrences of our target crime.


@Edgar: how do we feel about doing this? larger set of observations that correspond to our output class (50k, so we aren't running into as big an issue as before with almost 40 different output classes), and similar type crimes bucketed together? Wasn't sure what a good thresh value to choose would be. 

```{r logistic regression redux, message=FALSE, warning=FALSE}

#create column for output class
crime.data$output =  as.numeric(crime.data$Category == "ROBBERY" | crime.data$Category == "BURGLARY" | crime.data$Category == "LARCENY/THEFT")


#split data into training and test sets
set.seed(123)
data.split = split_data(crime.data)

tr_dat = data.split[[1]]
te_dat = data.split[[2]]

#generate model based on weather events, conditions, temp, day, and hour
fit = glm(output ~ Events + Humidity + Conditions + TemperatureF + DayOfWeek + Hour, data = tr_dat, family = binomial)

summary(fit)

#make predictions using test data
predicted = predict(fit, newdata=te_dat , type = "response")

hist(predicted)
#double density plot
plot(density(predicted[te_dat$output == 0]), col="green", main="Double Density Plot")
lines(density(predicted[te_dat$output == 1]), col="red")

#need new threshold
thresh = .025
abline(v=thresh, lty=2, col="blue")


predicted_vals = as.numeric(predicted < thresh)
actuals = te_dat$output

#generate and display confusion matrix
conf_matrix = table(actuals, predicted_vals)

conf_matrix

accuracy = mean(actuals == predicted_vals)

paste("Model Accuracy =", accuracy)
```

### New Model Analysis

```{r analysis of the logistic model redux, message=FALSE, warning=FALSE}
crime_predictions = prediction(predicted, te_dat$output)
roc = performance(crime_predictions, measure = "tpr", x.measure = "fpr")
prec_recall = performance(crime_predictions, measure="prec", x.measure="rec")
acc_curve = performance(crime_predictions, measure = "acc", x.measure = "cutoff")


plot(acc_curve, main="Accuracy Threshold Curve")
plot(prec_recall, main = "Precision Recall Curve")
plot(roc, main="Receiver Operating Characteristic",col="blue")
```

## Naive Bayes to classify Category
```{r naive bayes 1, message=FALSE, warning=FALSE}
crime.data$output = NULL

#Convert features to factors
crime.data$TemperatureF = as.factor(crime.data$TemperatureF) 
crime.data$lat = as.factor(crime.data$lat)
crime.data$lng = as.factor(crime.data$lng)
crime.data$Humidity = as.factor(crime.data$Humidity)
crime.data$Date = as.factor(crime.data$Date)
crime.data$Hour = as.factor(crime.data$Hour)
crime.data$IncidntNum = as.factor(crime.data$IncidntNum)
crime.data$PdId = as.factor(crime.data$PdId)

data.split = split_data(crime.data)

tr_dat = data.split[[1]]
te_dat = data.split[[2]]

fit2 = naiveBayes(Category ~ ., data = tr_dat)

predicted2 = predict(fit2, newdata = te_dat)

conf_matrx = table(predicted2, te_dat$Category)

accuracy = mean(predicted2 == te_dat$Category)

conf_matrx


paste("Model accuracy =", accuracy)
```


## Naive Bayes to classify tracts

Our other goal was to create a model that classified crimes based on which census tract it was most likely to present in. This model would, given a set of features, give a police officer from a given police district a notion of which tract to patrol a tract, given that it is potentially more prone to a specific sort of crime. To perform the classification, we used the Naive Bayes functionality of e1071. 

```{r naive bayes 2, message=FALSE, warning=FALSE,}

fit3 = naiveBayes(Tract ~ Events + Humidity + Conditions + TemperatureF + DayOfWeek + Hour + Category + PdDistrict , data = tr_dat)
summary(fit3)
actuals = te_dat$Tract
predicted3 = predict(fit3, newdata = te_dat)
conf_matrx = table(predicted3, actuals)

accuracy = mean(predicted3 == actuals)

paste("Model accuracy =", accuracy)

```
Once again the accuracy of the model was relatively low. We suspect this score took a hit because the model doesn't take into account that certain police districts would only expect to service a subset of the census tracts, not all of the tracts. Of course, the officers have jursidiction throughout San Francisco, but the majority of their work would lie within their districts. 
